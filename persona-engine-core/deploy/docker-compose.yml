version: "3.8"

services:
  # Layer 4: Intelligence (Inference Engine)
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia # Requires NVIDIA Container Toolkit
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8000:8000"
    command: >
      --model meta-llama/Meta-Llama-3-8B-Instruct
      --dtype auto
      --quantization awq
      --enable-prefix-caching
      --enable-chunked-prefill  # OPTIMIZATION: Prevents long prompts from stalling the queue
      --max-num-seqs 64         # OPTIMIZATION: Increase concurrency for enterprise scale
      --max-model-len 4096
      --gpu-memory-utilization 0.95
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    # Layer 5: Hot Memory (Redis)
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  # Layer 5: Long Term Memory (Qdrant)
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - ./qdrant_data:/qdrant/storage

  # Layer 2: Core API (Updated build context)
  core-api:
    build:
      context: ../
      dockerfile: deploy/Dockerfile
    ports:
      - "8080:8080"
    environment:
      - VLLM_ENDPOINT=http://vllm:8000/v1
      - REDIS_HOST=redis
      - QDRANT_HOST=qdrant
    depends_on:
      - vllm
      - redis
      - qdrant

volumes:
  redis_data:
